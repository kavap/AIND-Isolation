{
    "critiques": {
        "680": {
            "rubric_items": {
                "5510": {
                    "result": "failed",
                    "observation": "{\n    \"14. Submission includes heuristic_analysis.pdf\": {\n        \"traceback\": \"AssertionError: False is not true : You must submit a report named heuristic_analysis.pdf for review.\\n\"\n    }, \n    \"2. Test functionality of MinimaxPlayer.minimax()\": {\n        \"traceback\": \"AssertionError: False is not true : Your MinimaxAgent.minimax function did not visit every node in the game tree as player 1.  First check for off-by-one errors in your handling of the depth limiting. Then, especially if the number of nodes explored by your agent is too low, check everywhere you call to game.get_legal_moves() to make sure you are getting the legal moves for the appropriate player at each level of the game tree.  Finally, you may be using non-standard search optimizations that are not supported by the test cases.  The range of expansions accepted will vary slightly within the range indicated based on your termination condition.\\n\\nExpected number of visited nodes -- min: 2 max: 2\\nNumber of nodes your agent explored: 1\\n\\nTest Case Details:\\n------------------\\nHeuristic: open_move_score\\nDepth limit: 1\\nInitial Board State:\\n     0   1   2   3   4   5   6   7   8\\n\\r0  |   |   |   |   |   |   |   |   |   | \\n\\r1  |   |   |   | - |   |   |   |   |   | \\n\\r2  |   |   | - | - | - | - |   |   |   | \\n\\r3  |   |   | - | - | - | - | - |   |   | \\n\\r4  |   | - | - |   |   |   | - |   |   | \\n\\r5  |   |   | - | - | - | - | - |   |   | \\n\\r6  |   | - | - | - | - | - | 2 |   |   | \\n\\r7  |   |   |   | - | 1 |   |   |   |   | \\n\\r8  |   |   |   |   |   |   |   |   |   | \\n\\r\\ngame._board_state:\\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 43]\\n\\n\"\n    }, \n    \"15. Submission includes research_review.pdf\": {\n        \"traceback\": \"AssertionError: False is not true : You must submit a report named research_review.pdf for review.\\n\"\n    }, \n    \"6. Test the interface of AlphaBetaPlayer.get_move()\": {\n        \"traceback\": \"AssertionError: False is not true : The AlphaBetaPlayer.get_move() function failed as player 1 on an empty board. It should return coordinates on the game board for the location of the agent's next move. The move must be one of the legal moves on the current game board. Your function returned None which is not in the list of legal moves [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (0, 3), (1, 3), (2, 3), (3, 3), (4, 3), (5, 3), (6, 3), (7, 3), (8, 3), (0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4), (7, 4), (8, 4), (0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, 5), (6, 5), (7, 5), (8, 5), (0, 6), (1, 6), (2, 6), (3, 6), (4, 6), (5, 6), (6, 6), (7, 6), (8, 6), (0, 7), (1, 7), (2, 7), (3, 7), (4, 7), (5, 7), (6, 7), (7, 7), (8, 7), (0, 8), (1, 8), (2, 8), (3, 8), (4, 8), (5, 8), (6, 8), (7, 8), (8, 8)]\\n\"\n    }, \n    \"7. Test functionality of AlphaBetaPlayer.alphabeta()\": {\n        \"traceback\": \"AssertionError: Cut off search too early. (i.e., a correct implementation of alpha beta search did not prune at the same node as your agent when following the same node expansion order.) \\nAlpha: 4.0\\nBeta: inf\\nGame tree evaluation order:\\n[(2, 7)]\\n\\nNodes are shown with each layer sorted in the order the nodes were expanded\\nduring search.  All nodes in each successive layer are children of the\\nfurthest-right node in the parent layer above it.\\n\\nTest Case Details:\\n------------------\\nHeuristic: open_move_score\\nDepth limit: 1\\nInitial Board State:\\n     0   1   2   3   4   5   6   7   8\\n\\r0  |   |   |   |   |   |   |   |   |   | \\n\\r1  |   |   |   |   | - |   |   |   |   | \\n\\r2  |   |   | - |   | - | - | - |   |   | \\n\\r3  |   |   |   |   | - | - |   | - |   | \\n\\r4  |   | - | - | - | - | - | 1 |   |   | \\n\\r5  |   |   | - | - | - |   | - |   |   | \\n\\r6  |   |   | - | - |   |   | - |   |   | \\n\\r7  |   |   | 2 |   |   |   |   |   |   | \\n\\r8  |   |   |   |   |   |   |   |   |   | \\n\\r\\ngame._board_state:\\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 58]\\n\\n\"\n    }\n}"
                }
            }
        }
    }
}